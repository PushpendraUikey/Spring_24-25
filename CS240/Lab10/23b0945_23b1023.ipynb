{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import random \n",
    "import numpy as np \n",
    "from nltk.corpus import brown \n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/cs240lab/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/cs240lab/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('brown') \n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "tagged_sentences = brown.tagged_sents(tagset=\"universal\")\n",
    "\n",
    "sentences = list(tagged_sentences) \n",
    "random.seed(42)\n",
    "\n",
    "num_folds = 5 \n",
    "fold_size = len(sentences) // num_folds \n",
    "folds = []\n",
    "\n",
    "for i in range(num_folds):\n",
    "    start = i * fold_size\n",
    "    \n",
    "    if i == num_folds - 1:\n",
    "        fold = sentences[start:]\n",
    "    else: fold = sentences[start:start + fold_size]\n",
    "    folds.append(fold)\n",
    "\n",
    "unique_tags = sorted(set(tag for sent in sentences for _, tag in sent)) \n",
    "num_tags = len(unique_tags)\n",
    "\n",
    "tag_to_index = {tag: i for i, tag in enumerate(unique_tags)} \n",
    "index_to_tag = {i: tag for tag, i in tag_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hmm(train_sentences):\n",
    "    transition_counts = np.ones((num_tags, num_tags)) \n",
    "    emission_counts = {}\n",
    "    \n",
    "    for sentence in train_sentences:\n",
    "        prev_tag = None\n",
    "        for word, tag in sentence:\n",
    "            tag_idx = tag_to_index[tag]\n",
    "            if word not in emission_counts:\n",
    "                emission_counts[word] = np.ones(num_tags)\n",
    "            emission_counts[word][tag_idx] += 1\n",
    "            if prev_tag is not None:\n",
    "                prev_idx = tag_to_index[prev_tag]\n",
    "                transition_counts[prev_idx][tag_idx] += 1\n",
    "            prev_tag = tag\n",
    "\n",
    "    transition_probs = np.log(transition_counts / transition_counts.sum(axis=1, keepdims=True))\n",
    "    emission_probs = {word: np.log(counts / counts.sum()) for word, counts in emission_counts.items()}\n",
    "\n",
    "    return transition_probs, emission_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold: 1\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           .       0.24      1.00      0.38     30115\n",
      "         ADJ       0.91      0.48      0.63     18941\n",
      "         ADP       0.91      0.56      0.69     31048\n",
      "         ADV       0.92      0.54      0.68     10971\n",
      "        CONJ       0.99      0.53      0.69      7662\n",
      "         DET       0.95      0.62      0.75     29455\n",
      "        NOUN       0.93      0.46      0.62     66790\n",
      "         NUM       0.99      0.33      0.50      3974\n",
      "        PRON       0.95      0.65      0.78      8204\n",
      "         PRT       0.93      0.50      0.65      5791\n",
      "        VERB       0.97      0.56      0.71     37128\n",
      "           X       0.75      0.01      0.02       281\n",
      "\n",
      "    accuracy                           0.58    250360\n",
      "   macro avg       0.87      0.52      0.59    250360\n",
      "weighted avg       0.85      0.58      0.64    250360\n",
      "\n",
      "\n",
      "Fold: 2\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           .       0.25      1.00      0.39     59852\n",
      "         ADJ       0.91      0.50      0.64     38625\n",
      "         ADP       0.91      0.58      0.71     63323\n",
      "         ADV       0.92      0.55      0.69     23000\n",
      "        CONJ       0.99      0.54      0.70     16303\n",
      "         DET       0.95      0.64      0.76     59839\n",
      "        NOUN       0.92      0.49      0.64    127473\n",
      "         NUM       0.98      0.35      0.51      7090\n",
      "        PRON       0.95      0.67      0.78     16790\n",
      "         PRT       0.93      0.52      0.66     11476\n",
      "        VERB       0.97      0.57      0.72     75128\n",
      "           X       0.40      0.01      0.01       545\n",
      "\n",
      "    accuracy                           0.60    499444\n",
      "   macro avg       0.84      0.53      0.60    499444\n",
      "weighted avg       0.85      0.60      0.65    499444\n",
      "\n",
      "\n",
      "Fold: 3\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           .       0.24      1.00      0.39     90572\n",
      "         ADJ       0.92      0.50      0.64     60628\n",
      "         ADP       0.92      0.58      0.71    101879\n",
      "         ADV       0.93      0.55      0.69     35105\n",
      "        CONJ       0.99      0.53      0.69     25621\n",
      "         DET       0.95      0.64      0.76     92738\n",
      "        NOUN       0.92      0.50      0.65    195943\n",
      "         NUM       0.98      0.35      0.52     11588\n",
      "        PRON       0.95      0.67      0.79     24879\n",
      "         PRT       0.93      0.53      0.67     17161\n",
      "        VERB       0.97      0.57      0.72    115663\n",
      "           X       0.15      0.00      0.01       990\n",
      "\n",
      "    accuracy                           0.60    772767\n",
      "   macro avg       0.82      0.54      0.60    772767\n",
      "weighted avg       0.86      0.60      0.66    772767\n",
      "\n",
      "\n",
      "Fold: 4\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           .       0.26      1.00      0.41    118482\n",
      "         ADJ       0.91      0.50      0.65     73866\n",
      "         ADP       0.92      0.59      0.72    126332\n",
      "         ADV       0.93      0.56      0.70     45940\n",
      "        CONJ       0.99      0.55      0.71     32177\n",
      "         DET       0.95      0.65      0.77    116989\n",
      "        NOUN       0.92      0.51      0.65    241528\n",
      "         NUM       0.98      0.37      0.53     13802\n",
      "        PRON       0.95      0.70      0.81     35550\n",
      "         PRT       0.93      0.54      0.68     23316\n",
      "        VERB       0.97      0.59      0.73    150459\n",
      "           X       0.21      0.00      0.01      1205\n",
      "\n",
      "    accuracy                           0.62    979646\n",
      "   macro avg       0.83      0.55      0.61    979646\n",
      "weighted avg       0.86      0.62      0.67    979646\n",
      "\n",
      "\n",
      "Fold: 5\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           .       0.27      1.00      0.43    147565\n",
      "         ADJ       0.91      0.50      0.65     83721\n",
      "         ADP       0.91      0.60      0.72    144766\n",
      "         ADV       0.93      0.58      0.71     56239\n",
      "        CONJ       0.99      0.56      0.72     38151\n",
      "         DET       0.95      0.65      0.77    137019\n",
      "        NOUN       0.92      0.51      0.66    275558\n",
      "         NUM       0.98      0.38      0.55     14874\n",
      "        PRON       0.96      0.72      0.82     49334\n",
      "         PRT       0.93      0.55      0.69     29829\n",
      "        VERB       0.97      0.60      0.74    182750\n",
      "           X       0.30      0.01      0.01      1386\n",
      "\n",
      "    accuracy                           0.63   1161192\n",
      "   macro avg       0.84      0.55      0.62   1161192\n",
      "weighted avg       0.85      0.63      0.67   1161192\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def viterbi(sentence, transition_probs, emission_probs): \n",
    "    V = [{}] \n",
    "    for i, tag in enumerate(unique_tags): \n",
    "        V[0][tag] = {\"prob\": emission_probs.get(sentence[0], np.full(num_tags, -np.inf))[i] + np.log(1/num_tags), \"prev\": None}\n",
    "\n",
    "    for t in range(1, len(sentence)):\n",
    "        V.append({})\n",
    "        for i, tag in enumerate(unique_tags):\n",
    "            max_tr_prob = max(V[t-1][prev_tag][\"prob\"] + transition_probs[tag_to_index[prev_tag], i] for prev_tag in unique_tags)\n",
    "            for prev_tag in unique_tags:\n",
    "                if V[t-1][prev_tag][\"prob\"] + transition_probs[tag_to_index[prev_tag], i] == max_tr_prob:\n",
    "                    max_prob = max_tr_prob + emission_probs.get(sentence[t], np.full(num_tags, -np.inf))[i]\n",
    "                    V[t][tag] = {\"prob\": max_prob, \"prev\": prev_tag}\n",
    "                    break\n",
    "\n",
    "    opt = []\n",
    "    max_prob = max(value[\"prob\"] for value in V[-1].values())\n",
    "    previous = None\n",
    "\n",
    "    for tag, data in V[-1].items():\n",
    "        if data[\"prob\"] == max_prob:\n",
    "            opt.append(tag)\n",
    "            previous = tag\n",
    "            break\n",
    "\n",
    "    for t in range(len(V) - 2, -1, -1):\n",
    "        opt.insert(0, V[t + 1][previous][\"prev\"])\n",
    "        previous = V[t + 1][previous][\"prev\"]\n",
    "\n",
    "    return opt\n",
    "\n",
    "actual_tags, predicted_tags = [], [] \n",
    "for i in range(num_folds): \n",
    "    print(\"\\nFold:\", i+1) \n",
    "    test_set = folds[i] \n",
    "    train_set = [sent for j in range(num_folds) if j != i for sent in folds[j]]\n",
    "\n",
    "    trans_probs, emiss_probs = train_hmm(train_set)\n",
    "\n",
    "    for sentence in test_set:\n",
    "        words = [word for word, _ in sentence]\n",
    "        actual = [tag for _, tag in sentence]\n",
    "        predicted = viterbi(words, trans_probs, emiss_probs)\n",
    "        actual_tags.extend(actual)\n",
    "        predicted_tags.extend(predicted)\n",
    "\n",
    "    print(\"\\nClassification Report:\") \n",
    "    print(classification_report(actual_tags, predicted_tags, labels=unique_tags))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i have been using this for more than ten years 10 guy\n",
      "\n",
      " pos Tags for input sentence: ['PRON', 'VERB', 'VERB', 'VERB', 'DET', 'ADP', 'ADJ', 'ADP', 'NUM', 'NOUN', 'NUM', 'NOUN']\n"
     ]
    }
   ],
   "source": [
    "def pos_tag_sentence(sentence): \n",
    "    trans_probs, emiss_probs = train_hmm(sentences) \n",
    "    return viterbi(sentence.split(), trans_probs, emiss_probs)\n",
    "\n",
    "example_sentence = \"I have been using this for more than ten years 10 guy\" \n",
    "m=example_sentence.lower()\n",
    "print(m)\n",
    "\n",
    "print(\"\\n pos Tags for input sentence:\", pos_tag_sentence(m))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
